{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: Imports\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# STEP 1: Custom Dataset for Bubble Images\n",
    "import random\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "import random\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class BubbleDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths, augment=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def center_crop(self, img, target_width=750, target_height=554):\n",
    "        w, h = img.size\n",
    "        left = (w - target_width) // 2\n",
    "        top = (h - target_height) // 2\n",
    "        right = left + target_width\n",
    "        bottom = top + target_height\n",
    "        return img.crop((left, top, right, bottom))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load paths\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "\n",
    "        # Load images\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        label = Image.open(label_path)\n",
    "\n",
    "        # Keep original image before cropping/resizing\n",
    "        original_image_tensor = TF.to_tensor(image) # [3, H, W]\n",
    "\n",
    "        # Apply center crop (only width changes)\n",
    "        image = self.center_crop(image, target_width=750)\n",
    "        label = self.center_crop(label, target_width=750)\n",
    "\n",
    "        # Resize to model input\n",
    "        image = TF.resize(image, (256, 256))\n",
    "        label = TF.resize(label, (256, 256), interpolation=Image.NEAREST)\n",
    "\n",
    "        # Data Augmentation (if enabled)\n",
    "        if self.augment:\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.hflip(image)\n",
    "                label = TF.hflip(label)\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.vflip(image)\n",
    "                label = TF.vflip(label)\n",
    "            if random.random() > 0.5:\n",
    "                angle = random.uniform(-5, 5)\n",
    "                image = TF.rotate(image, angle)\n",
    "                label = TF.rotate(label, angle, interpolation=Image.NEAREST)\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.adjust_brightness(image, random.uniform(0.9, 1.1))\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.adjust_contrast(image, random.uniform(0.9, 1.1))\n",
    "            if random.random() > 0.5:\n",
    "                i, j, h, w = transforms.RandomResizedCrop.get_params(\n",
    "                    image, scale=(0.9, 1.0), ratio=(1.0, 1.0))\n",
    "                image = TF.resized_crop(image, i, j, h, w, (256, 256))\n",
    "                label = TF.resized_crop(label, i, j, h, w, (256, 256), interpolation=Image.NEAREST)\n",
    "            if random.random() > 0.5:\n",
    "                img_tensor = TF.to_tensor(image)\n",
    "                noise = torch.randn_like(img_tensor) * 0.01\n",
    "                img_tensor = (img_tensor + noise).clamp(0, 1)\n",
    "                image = TF.to_pil_image(img_tensor)\n",
    "\n",
    "        # Final conversion to tensor\n",
    "        # image = TF.to_tensor(image).expand(3, -1, -1)\n",
    "        image = TF.to_tensor(image)\n",
    "        image = image.expand(3, -1, -1)\n",
    "        label = TF.pil_to_tensor(label).squeeze().long()\n",
    "        label = (label > 127).long()\n",
    "\n",
    "        return image, label, original_image_tensor, image_path, label_path\n",
    "\n",
    "# STEP 2: Parsing and Splitting Data Based on Dataset Number\n",
    "all_images = sorted(glob.glob('../Data/US_2/*.jpg'))\n",
    "all_labels = [img_path.replace('US', 'Label').replace('.jpg', '.png') for img_path in all_images]\n",
    "\n",
    "# Extract dataset number (last digit before .jpg)\n",
    "def extract_dataset_number(path):\n",
    "    return int(path.split('_')[-1].split('.')[0])\n",
    "\n",
    "groups = [extract_dataset_number(p) for p in all_images]\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=1/6)\n",
    "print(\"Number of unique groups:\", len(np.unique(groups)))\n",
    "train_idx, val_idx = next(splitter.split(all_images, groups=groups))\n",
    "\n",
    "# Optional: If you want to merge train & val into train (not common)\n",
    "# train_idx = np.concatenate([train_idx, val_idx])\n",
    "\n",
    "train_images = [all_images[i] for i in train_idx]\n",
    "train_labels = [all_labels[i] for i in train_idx]\n",
    "val_images = [all_images[i] for i in val_idx]\n",
    "val_labels = [all_labels[i] for i in val_idx]\n",
    "\n",
    "print(np.array(set([extract_dataset_number(p) for p in train_images])))\n",
    "print(np.array(set([extract_dataset_number(p) for p in val_images])))\n",
    "\n",
    "print(\"Sample mapping:\")\n",
    "for img, lbl in zip(train_images[:3], train_labels[:3]):\n",
    "    print(f\"{img}  -->  {lbl}\")\n",
    "\n",
    "\n",
    "# STEP 3: Transforms\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "label_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=Image.NEAREST),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Lambda(lambda x: x.squeeze().long())\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = BubbleDataset(train_images, train_labels, augment=True)\n",
    "val_dataset = BubbleDataset(val_images, val_labels, augment=False)\n",
    "\n",
    "train_datasets = sorted(set(extract_dataset_number(p) for p in train_images))\n",
    "val_datasets = sorted(set(extract_dataset_number(p) for p in val_images))\n",
    "\n",
    "print(\"Train dataset numbers:\", train_datasets)\n",
    "print(\"Validation dataset numbers:\", val_datasets)\n",
    "\n",
    "\n",
    "# STEP 4: Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# STEP 5: Check output of new dataset \n",
    "img_batch, lbl_batch, orig_img_batch, _, _ = next(iter(train_loader))\n",
    "\n",
    "print(\"Image shape:\", img_batch.shape)        # [B, 3, 256, 256]\n",
    "print(\"Label shape:\", lbl_batch.shape)        # [B, 256, 256]\n",
    "print(\"Original Image shape:\", orig_img_batch.shape)  # [B, 3, H, W]  <- original untouched\n",
    "\n",
    "print(\"Label dtype:\", lbl_batch.dtype)        # should be torch.int64\n",
    "print(\"Label values:\", lbl_batch.unique())    # should be tensor([0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "\n",
    "def get_torchvision_deeplabv3(num_classes=1):\n",
    "    try:\n",
    "        weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "        model = deeplabv3_resnet101(weights=weights)\n",
    "        \n",
    "        # Modify the first convolution layer for grayscale input (1 channel)\n",
    "        # model.backbone.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Modify the classifier for custom number of classes\n",
    "        model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        \n",
    "        # Move to GPU if available and requested\n",
    "        use_cuda = True\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            print(\"Model successfully moved to CUDA\")\n",
    "        else:\n",
    "            print(\"CUDA not used - running on CPU\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during model setup:\", e)\n",
    "        return None\n",
    "\n",
    "model = get_torchvision_deeplabv3(num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 0: Imports ---\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# --- STEP 1: Load Test Data ---\n",
    "test_images = sorted(glob.glob('../Data/US_Test_2023April7/*.jpg'))\n",
    "test_labels = sorted(glob.glob('../Data/Label_Test_2023April7/*.png'))\n",
    "\n",
    "test_dataset = BubbleDataset(test_images, test_labels, augment=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "train_images = sorted(glob.glob('../Data/US_2/*.jpg'))\n",
    "train_labels = sorted(glob.glob('../Data/Label_2/*.png'))\n",
    "\n",
    "train_dataset = BubbleDataset(train_images, train_labels, augment=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "# --- STEP 2: Load Model ---\n",
    "# model.load_state_dict(torch.load(\"best_model_final.pth\"))\n",
    "# model.eval()\n",
    "\n",
    "checkpoint = torch.load(\"../code_files/checkpoints/TorchvisionDeepLabV3_DiceFocalLoss_Dice0.5_Tversky0.5_Focal0.6_Epochs5_LR0.0003/best.pth.tar\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "print(\"=> Checkpoint loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision.transforms.functional as TF\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image, ImageOps\n",
    "# import random\n",
    "\n",
    "# # --- Visualization Helpers ---\n",
    "# def show_image(img, title=\"Image\"):\n",
    "#     w, h = img.size\n",
    "#     plt.figure(figsize=(w / 100, h / 100))\n",
    "#     cmap = \"gray\" if img.mode == \"L\" else None\n",
    "#     plt.imshow(img, cmap=cmap)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.title(title)\n",
    "#     plt.show()\n",
    "\n",
    "# def recover_original(img, final_width=1024, final_height=256, cropped_width=750, cropped_height=554):\n",
    "#     # Step 1: Resize back to cropped size\n",
    "#     img_resized = TF.resize(img, [cropped_height, cropped_width], interpolation=Image.NEAREST)\n",
    "\n",
    "#     # Step 2: Pad to original size\n",
    "#     pad_left = (final_width - cropped_width) // 2\n",
    "#     pad_right = final_width - cropped_width - pad_left\n",
    "#     pad_top = (final_height - cropped_height) // 2\n",
    "#     pad_bottom = final_height - cropped_height - pad_top\n",
    "\n",
    "#     img_padded = ImageOps.expand(img_resized, (pad_left, pad_top, pad_right, pad_bottom), fill=0)\n",
    "\n",
    "#     return img_padded\n",
    "\n",
    "# def visualize_final(img_path, pred_tensor, gt_tensor, idx, label_path):\n",
    "#     orig_img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "#     # Convert pred and gt to PIL images\n",
    "#     pred_pil = TF.to_pil_image(pred_tensor.byte() * 255)\n",
    "#     gt_pil = TF.to_pil_image(gt_tensor.byte() * 255)\n",
    "\n",
    "#     # Recover to original size\n",
    "#     pred_recovered = recover_original(pred_pil)\n",
    "#     gt_recovered = recover_original(gt_pil)\n",
    "\n",
    "#     print(f\"\\nSample #{idx}\")\n",
    "#     print(f\"Image Path: {img_path}\")\n",
    "#     print(f\"Label Path: {label_path}\")\n",
    "#     show_image(orig_img, title=\"Original Image (1024x256)\")\n",
    "#     show_image(pred_recovered, title=\"Prediction Mask (Recovered to 1024x256)\")\n",
    "#     show_image(gt_recovered, title=\"Ground Truth Mask (Recovered to 1024x256)\")\n",
    "\n",
    "# # --- Inference + Visualization Loop ---\n",
    "# all_samples = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for imgs, masks, orig_imgs, _, _ in test_loader:\n",
    "#         imgs, masks = imgs.cuda(), masks.cuda()\n",
    "#         preds = torch.argmax(model(imgs)['out'], dim=1)\n",
    "\n",
    "#         for i in range(imgs.size(0)):\n",
    "#             all_samples.append((\n",
    "#                 imgs[i].cpu(),\n",
    "#                 masks[i].cpu(),\n",
    "#                 preds[i].cpu(),\n",
    "#                 orig_imgs[i].cpu(),\n",
    "#                 test_dataset.image_paths[len(all_samples)],\n",
    "#                 test_dataset.label_paths[len(all_samples)]\n",
    "#             ))\n",
    "\n",
    "# print(f\"Total samples = {len(all_samples)}\")\n",
    "\n",
    "# # Visualize Random 4 Samples\n",
    "# for idx, (img, gt, pred, orig_img, img_path, label_path) in enumerate(random.sample(all_samples, 4)):\n",
    "#     visualize_final(img_path, pred, gt, idx, label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "\n",
    "# Redefining same function again for sake of simplicity\n",
    "def recover_original(img, final_width=1024, final_height=256, cropped_width=750, cropped_height=554):\n",
    "    # Step 1: Resize back to cropped size\n",
    "    img_resized = TF.resize(img, [cropped_height, cropped_width], interpolation=Image.NEAREST)\n",
    "\n",
    "    # Step 2: Pad to original size\n",
    "    pad_left = (final_width - cropped_width) // 2\n",
    "    pad_right = final_width - cropped_width - pad_left\n",
    "    pad_top = (final_height - cropped_height) // 2\n",
    "    pad_bottom = final_height - cropped_height - pad_top\n",
    "\n",
    "    img_padded = ImageOps.expand(img_resized, (pad_left, pad_top, pad_right, pad_bottom), fill=0)\n",
    "\n",
    "    return img_padded\n",
    "\n",
    "def visualize_final(img_path, pred_tensor, gt_tensor, idx, label_path):\n",
    "    orig_img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    pred_pil = TF.to_pil_image(pred_tensor.byte() * 255)\n",
    "    gt_pil = TF.to_pil_image(gt_tensor.byte() * 255)\n",
    "\n",
    "    pred_recovered = recover_original(pred_pil)\n",
    "    gt_recovered = recover_original(gt_pil)\n",
    "\n",
    "    print(f\"\\nSample #{idx}\")\n",
    "    print(f\"Image Path: {img_path}\")\n",
    "    print(f\"Label Path: {label_path}\")\n",
    "\n",
    "    # --- Plot in a row of 3 ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    images = [orig_img, pred_recovered, gt_recovered]\n",
    "    titles = ['Original Image', 'Prediction Mask', 'Ground Truth Mask']\n",
    "\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img, cmap='gray' if img.mode == 'L' else None)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Inference + Visualization Loop ---\n",
    "all_samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, masks, orig_imgs, _, _ in test_loader:\n",
    "        imgs, masks = imgs.cuda(), masks.cuda()\n",
    "        preds = torch.argmax(model(imgs)['out'], dim=1)\n",
    "\n",
    "        for i in range(imgs.size(0)):\n",
    "            all_samples.append((\n",
    "                imgs[i].cpu(),\n",
    "                masks[i].cpu(),\n",
    "                preds[i].cpu(),\n",
    "                orig_imgs[i].cpu(),\n",
    "                test_dataset.image_paths[len(all_samples)],\n",
    "                test_dataset.label_paths[len(all_samples)]\n",
    "            ))\n",
    "\n",
    "print(f\"Total samples = {len(all_samples)}\")\n",
    "\n",
    "# Visualize Random 10 Samples\n",
    "# for idx, (img, gt, pred, orig_img, img_path, label_path) in enumerate(random.sample(all_samples, 10)):\n",
    "#     visualize_final(img_path, pred, gt, idx, label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Extract pulse number from image filename\n",
    "def extract_pulse(filename):\n",
    "    base = os.path.basename(filename)\n",
    "    pulse_match = re.search(r'US(\\d+)', base)\n",
    "    pulse = int(pulse_match.group(1)) if pulse_match else -1\n",
    "    return pulse\n",
    "\n",
    "# Step 1: Map pulses to their sample indices\n",
    "pulse_idx_pairs = [\n",
    "    (extract_pulse(img_path), idx)\n",
    "    for idx, (_, _, _, _, img_path, _) in enumerate(all_samples)\n",
    "]\n",
    "\n",
    "# Step 2: Sort by pulse\n",
    "pulse_idx_pairs_sorted = sorted(pulse_idx_pairs, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Pick every nth unique pulse\n",
    "n = 134\n",
    "selected_indices = [idx for i, (_, idx) in enumerate(pulse_idx_pairs_sorted) if i % n == 0]\n",
    "\n",
    "# Step 4: Select corresponding samples\n",
    "selected_samples = [all_samples[idx] for idx in selected_indices]\n",
    "\n",
    "# Step 5: Plot\n",
    "fig, axes = plt.subplots(len(selected_samples), 3, figsize=(15, 3 * len(selected_samples)))\n",
    "\n",
    "for row_idx, (img, gt, pred, orig_img, img_path, label_path) in enumerate(selected_samples):\n",
    "    orig_img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    pred_pil = TF.to_pil_image(pred.byte() * 255)\n",
    "    gt_pil = TF.to_pil_image(gt.byte() * 255)\n",
    "\n",
    "    pred_recovered = recover_original(pred_pil)\n",
    "    gt_recovered = recover_original(gt_pil)\n",
    "\n",
    "    images = [orig_img, pred_recovered, gt_recovered]\n",
    "\n",
    "    pulse = extract_pulse(img_path)\n",
    "    titles = [f'B-Mode Image - {pulse}', 'Predicted Mask', 'Ground Truth']\n",
    "\n",
    "    # print(f\"\\nSample #{row_idx}\")\n",
    "    # print(f\"Pulse: {pulse}\")\n",
    "    # print(f\"Image Path: {img_path}\")\n",
    "    # print(f\"Label Path: {label_path}\")\n",
    "\n",
    "    for col, (ax, img, title) in enumerate(zip(axes[row_idx], images, titles)):\n",
    "        ax.imshow(img, cmap='gray' if img.mode == 'L' else None)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulse_list = [extract_pulse(img_path) for (_, _, _, _, img_path, _) in all_samples]\n",
    "\n",
    "unique_pulses = sorted(list(set(pulse_list)))\n",
    "\n",
    "print(f\"Total Unique Pulses = {len(unique_pulses)}\")\n",
    "print(\"Unique Pulses =\", unique_pulses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "# --- Main Code ---\n",
    "# Desired pulses to plot\n",
    "desired_pulses = [1, 50, 100]\n",
    "\n",
    "# Build a map from pulse value to sample index (first occurrence only)\n",
    "pulse_idx_map = {}\n",
    "for idx, (_, _, _, _, img_path, _) in enumerate(all_samples):\n",
    "    pulse = extract_pulse(img_path)\n",
    "    if pulse in desired_pulses and pulse not in pulse_idx_map:\n",
    "        pulse_idx_map[pulse] = idx  # Pick first occurrence\n",
    "\n",
    "print(\"Pulse to Index Mapping:\", pulse_idx_map)\n",
    "\n",
    "# Prepare selected samples based on the desired pulses\n",
    "selected_samples = [all_samples[pulse_idx_map[pulse]] for pulse in desired_pulses if pulse in pulse_idx_map]\n",
    "\n",
    "# Create a subplot grid with as many rows as selected samples and 3 columns.\n",
    "fig, axes = plt.subplots(len(selected_samples), 3, figsize=(15, 3 * len(selected_samples)))\n",
    "\n",
    "# If only one row exists, ensure axes is 2D for consistency.\n",
    "if len(selected_samples) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Set the column titles (only once, on the top row)\n",
    "column_titles = ['B-Mode Image', 'Predicted Mask', 'Ground Truth']\n",
    "for col_idx, title in enumerate(column_titles):\n",
    "    axes[0][col_idx].set_title(title, fontsize=20)\n",
    "\n",
    "# Now plot each sample without additional row titles;\n",
    "# Instead, annotate each B-Mode image with \"No. of Pulses: X\"\n",
    "for row_idx, (img, gt, pred, orig_img, img_path, label_path) in enumerate(selected_samples):\n",
    "    # Load and prepare images\n",
    "    orig_img = Image.open(img_path).convert(\"RGB\")\n",
    "    pred_pil = TF.to_pil_image(pred.byte() * 255)\n",
    "    gt_pil = TF.to_pil_image(gt.byte() * 255)\n",
    "    \n",
    "    pred_recovered = recover_original(pred_pil)\n",
    "    gt_recovered = recover_original(gt_pil)\n",
    "    \n",
    "    images = [orig_img, pred_recovered, gt_recovered]\n",
    "    pulse = extract_pulse(img_path)\n",
    "    \n",
    "    print(f\"\\nSample #{row_idx}\")\n",
    "    print(f\"Pulse: {pulse}\")\n",
    "    print(f\"Image Path: {img_path}\")\n",
    "    \n",
    "    for col_idx, (ax, im) in enumerate(zip(axes[row_idx], images)):\n",
    "        ax.imshow(im, cmap='gray' if im.mode == 'L' else None)\n",
    "        ax.axis('off')\n",
    "        # For the B-Mode image (first column), add the pulse count as an annotation.\n",
    "        if col_idx == 0:\n",
    "            ax.text(-0.1, 0.5, f'No. of Pulses: {pulse}',\n",
    "                    transform=ax.transAxes, fontsize=16, ha='center', va='center',\n",
    "                    rotation=90)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
